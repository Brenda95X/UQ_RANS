{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhxch\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from utils_preprocess import make_dataframe\n",
    "from models import NetworkStructure\n",
    "from utils_train import ModelTrainer, plot_loss_history, calc_r2\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train: 1126\n",
      "N_val: 282\n",
      "N_test: 192\n"
     ]
    }
   ],
   "source": [
    "'''Set numpy random seed'''\n",
    "\n",
    "numpy_seed = 999\n",
    "np.random.seed(numpy_seed)\n",
    "\n",
    "\n",
    "'''Prepare channel flow data'''\n",
    "\n",
    "Retau_train = [1000, 2000, 5200] \n",
    "Retau_test = [550]\n",
    "\n",
    "df_train = pd.concat([make_dataframe(Retau) for Retau in Retau_train])\n",
    "df_test = pd.concat([make_dataframe(Retau) for Retau in Retau_test])\n",
    "\n",
    "df_train['a'] = df_train[['a_uv', 'a_uu', 'a_vv', 'a_ww']].values.tolist()\n",
    "df_test['a'] = df_test[['a_uv', 'a_uu', 'a_vv', 'a_ww']].values.tolist()\n",
    "\n",
    "# scale up target variable in order for stable training, will consider implementing automatic scaler \n",
    "df_train['a_uv'] *= 1e3   \n",
    "df_test['a_uv'] *= 1e3 \n",
    "\n",
    "\n",
    "'''Split training data into training and validation data'''\n",
    "\n",
    "val_fraction = 0.2\n",
    "df_train, df_val = train_test_split(df_train, test_size=val_fraction, shuffle=True) \n",
    "\n",
    "# # use following approach to imitate pytorch random_split function (which was used in earlier experiments)\n",
    "# from torch import randperm \n",
    "# torch.manual_seed(999)\n",
    "# indices = randperm(len(df_train)).tolist()\n",
    "# train_indices = indices[:int((1-val_fraction)*len(df_train))]\n",
    "# val_indices = indices[int((1-val_fraction)*len(df_train)):]\n",
    "# df_val = df_train.iloc[val_indices]\n",
    "# df_train = df_train.iloc[train_indices]\n",
    "\n",
    "\n",
    "'''Print number of data points'''\n",
    "\n",
    "print('N_train: {}\\nN_val: {}\\nN_test: {}'.format(len(df_train), len(df_val), len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y+</th>\n",
       "      <th>y</th>\n",
       "      <th>index</th>\n",
       "      <th>du_dy</th>\n",
       "      <th>a_uv</th>\n",
       "      <th>a_uu</th>\n",
       "      <th>a_vv</th>\n",
       "      <th>a_ww</th>\n",
       "      <th>Re_tau</th>\n",
       "      <th>DU_DY</th>\n",
       "      <th>Y</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>54.730663</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>56</td>\n",
       "      <td>9.318907</td>\n",
       "      <td>-1.626941</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>-0.003841</td>\n",
       "      <td>-0.001085</td>\n",
       "      <td>5185.897</td>\n",
       "      <td>[215.14682814005528, 215.1435270334813, 215.13...</td>\n",
       "      <td>[0.0, 1.3710713922450184e-05, 4.16995405351281...</td>\n",
       "      <td>[-0.0016269412300123857, 0.004926263300954793,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>3252.192145</td>\n",
       "      <td>0.627122</td>\n",
       "      <td>576</td>\n",
       "      <td>0.196459</td>\n",
       "      <td>-0.636307</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>-0.000649</td>\n",
       "      <td>-0.000392</td>\n",
       "      <td>5185.897</td>\n",
       "      <td>[215.14682814005528, 215.1435270334813, 215.13...</td>\n",
       "      <td>[0.0, 1.3710713922450184e-05, 4.16995405351281...</td>\n",
       "      <td>[-0.0006363065051346082, 0.001040375549731483,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>4942.830131</td>\n",
       "      <td>0.953129</td>\n",
       "      <td>744</td>\n",
       "      <td>0.029074</td>\n",
       "      <td>-0.079696</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>5185.897</td>\n",
       "      <td>[215.14682814005528, 215.1435270334813, 215.13...</td>\n",
       "      <td>[0.0, 1.3710713922450184e-05, 4.16995405351281...</td>\n",
       "      <td>[-7.969594778035911e-05, 0.0003559033069961584...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2547.326692</td>\n",
       "      <td>0.491203</td>\n",
       "      <td>500</td>\n",
       "      <td>0.251217</td>\n",
       "      <td>-0.869860</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>-0.000926</td>\n",
       "      <td>-0.000527</td>\n",
       "      <td>5185.897</td>\n",
       "      <td>[215.14682814005528, 215.1435270334813, 215.13...</td>\n",
       "      <td>[0.0, 1.3710713922450184e-05, 4.16995405351281...</td>\n",
       "      <td>[-0.000869859846102128, 0.0014527322323839662,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>319.879688</td>\n",
       "      <td>0.061683</td>\n",
       "      <td>158</td>\n",
       "      <td>1.741034</td>\n",
       "      <td>-1.599301</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>-0.002799</td>\n",
       "      <td>-0.001257</td>\n",
       "      <td>5185.897</td>\n",
       "      <td>[215.14682814005528, 215.1435270334813, 215.13...</td>\n",
       "      <td>[0.0, 1.3710713922450184e-05, 4.16995405351281...</td>\n",
       "      <td>[-0.001599301010904181, 0.004055674757247312, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              y+         y  index     du_dy      a_uv      a_uu      a_vv  \\\n",
       "56     54.730663  0.010554     56  9.318907 -1.626941  0.004926 -0.003841   \n",
       "576  3252.192145  0.627122    576  0.196459 -0.636307  0.001040 -0.000649   \n",
       "744  4942.830131  0.953129    744  0.029074 -0.079696  0.000356 -0.000185   \n",
       "500  2547.326692  0.491203    500  0.251217 -0.869860  0.001453 -0.000926   \n",
       "158   319.879688  0.061683    158  1.741034 -1.599301  0.004056 -0.002799   \n",
       "\n",
       "         a_ww    Re_tau                                              DU_DY  \\\n",
       "56  -0.001085  5185.897  [215.14682814005528, 215.1435270334813, 215.13...   \n",
       "576 -0.000392  5185.897  [215.14682814005528, 215.1435270334813, 215.13...   \n",
       "744 -0.000171  5185.897  [215.14682814005528, 215.1435270334813, 215.13...   \n",
       "500 -0.000527  5185.897  [215.14682814005528, 215.1435270334813, 215.13...   \n",
       "158 -0.001257  5185.897  [215.14682814005528, 215.1435270334813, 215.13...   \n",
       "\n",
       "                                                     Y  \\\n",
       "56   [0.0, 1.3710713922450184e-05, 4.16995405351281...   \n",
       "576  [0.0, 1.3710713922450184e-05, 4.16995405351281...   \n",
       "744  [0.0, 1.3710713922450184e-05, 4.16995405351281...   \n",
       "500  [0.0, 1.3710713922450184e-05, 4.16995405351281...   \n",
       "158  [0.0, 1.3710713922450184e-05, 4.16995405351281...   \n",
       "\n",
       "                                                     a  \n",
       "56   [-0.0016269412300123857, 0.004926263300954793,...  \n",
       "576  [-0.0006363065051346082, 0.001040375549731483,...  \n",
       "744  [-7.969594778035911e-05, 0.0003559033069961584...  \n",
       "500  [-0.000869859846102128, 0.0014527322323839662,...  \n",
       "158  [-0.001599301010904181, 0.004055674757247312, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch_seed = 999\n",
    "torch.manual_seed(torch_seed)\n",
    "\n",
    "def evaluate(trainer, df):\n",
    "    preds, targets = trainer.predict(df)\n",
    "    return {r'$R^2$': calc_r2(targets, preds)}\n",
    "\n",
    "'''Set target variable'''\n",
    "target_label = ['a_uv'] # or ['a'] (output_len = 1 if 'a_uv'; output_len = 4 if 'a')\n",
    "\n",
    "\n",
    "'''Define model'''\n",
    "# model_name: ['MLP', 'MLP-Prior', 'MLP-BC', 'MLP-Re', 'MLP-NL', 'MLP-Re-Prior', 'MLP-BC-Re', 'MLP-BC-Re-Prior', 'MLP-BC-Re-NL']\n",
    "# model_kwargs is a dictionary of model hyperparameters  \n",
    "# Re models need 'Re_input_layers', a tuple of layer indices that specifies at which layers to input Re_tau\n",
    "# NL models need 'alphanet_structure' (structure of the network predicting alpha from y+) \n",
    "# and 'alpha0' (boundary condition for alpha at y+ = 0, can only be 0 or 1, default is 1) \n",
    "# Prior models need 'scale'\n",
    "\n",
    "model_MLP = 'MLP'\n",
    "model_MLP_kwargs = {'structure': \n",
    "                    NetworkStructure().set_num_layers(5).set_num_nodes(50).set_output_len(1).set_activation(nn.ELU())}\n",
    "\n",
    "model_MLP_Re = 'MLP-Re'\n",
    "model_MLP_Re_kwargs = {'structure': \n",
    "                       NetworkStructure().set_num_layers(5).set_num_nodes(50).set_output_len(1).set_activation(nn.ELU()),\n",
    "                       'Re_input_layers': (3,)}\n",
    "\n",
    "model_MLP_BC_Re = 'MLP-BC-Re'\n",
    "model_MLP_BC_Re_kwargs = {'structure': \n",
    "                       NetworkStructure().set_num_layers(5).set_num_nodes(50).set_output_len(1).set_activation(nn.ELU()),\n",
    "                       'Re_input_layers': (3,)}\n",
    "\n",
    "model_MLP_BC_Re_NL = 'MLP-BC-Re-NL'\n",
    "model_MLP_BC_Re_NL_kwargs = {'structure': \n",
    "                       NetworkStructure().set_num_layers(5).set_num_nodes(50).set_output_len(1).set_activation(nn.ELU()),\n",
    "                       'Re_input_layers': (3,),\n",
    "                       'alphanet_structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_activation(nn.ELU()),\n",
    "                       'alpha0': 1}\n",
    "\n",
    "model1 = 'MLP-BC-Re-Prior'\n",
    "model1_kwargs = {'structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_output_len(1).set_activation(nn.ELU()),\n",
    "     'Re_input_layers': (3,),\n",
    "#     'alphanet_structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_activation(nn.ELU()),\n",
    "#     'alpha0': 1,\n",
    "     'scale': 1,  \n",
    "               }\n",
    "\n",
    "model2 = 'MLP-BC-Re-Prior'\n",
    "model2_kwargs = {'structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_output_len(1).set_activation(nn.ELU()),\n",
    "     'Re_input_layers': (3,),\n",
    "#     'alphanet_structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_activation(nn.ELU()),\n",
    "#     'alpha0': 1,\n",
    "     'scale': 2,  \n",
    "               }\n",
    "\n",
    "model3 = 'MLP-BC-Re-Prior'\n",
    "model3_kwargs = {'structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_output_len(1).set_activation(nn.ELU()),\n",
    "     'Re_input_layers': (3,),\n",
    "#     'alphanet_structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_activation(nn.ELU()),\n",
    "#     'alpha0': 1,\n",
    "     'scale': 3,  \n",
    "               }\n",
    "\n",
    "model5 = 'MLP-BC-Re-Prior'\n",
    "model5_kwargs = {'structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_output_len(1).set_activation(nn.ELU()),\n",
    "     'Re_input_layers': (3,),\n",
    "#     'alphanet_structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_activation(nn.ELU()),\n",
    "#     'alpha0': 1,\n",
    "     'scale': 5,  \n",
    "               }\n",
    "\n",
    "model7 = 'MLP-BC-Re-Prior'\n",
    "model7_kwargs = {'structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_output_len(1).set_activation(nn.ELU()),\n",
    "     'Re_input_layers': (3,),\n",
    "#     'alphanet_structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_activation(nn.ELU()),\n",
    "#     'alpha0': 1,\n",
    "     'scale': 7,  \n",
    "               }\n",
    "\n",
    "model10 = 'MLP-BC-Re-Prior'\n",
    "model10_kwargs = {'structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_output_len(1).set_activation(nn.ELU()),\n",
    "     'Re_input_layers': (3,),\n",
    "#     'alphanet_structure': NetworkStructure().set_num_layers(5).set_num_nodes(50).set_activation(nn.ELU()),\n",
    "#     'alpha0': 1,\n",
    "     'scale': 10}\n",
    "\n",
    "'''Define test case'''\n",
    "df=df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       1.12         1.72342         1.59737         2.79137\n",
      "   200       0.18         0.18482         0.16389         0.65650\n",
      "   400       0.24         0.09830         0.09429         0.41235\n",
      "   600       0.20         0.06128         0.05557         0.32667\n",
      "   800       0.17         0.04232         0.03529         0.29910\n",
      "  1000       0.17         0.03757         0.03087         0.30539\n",
      "  1200       0.17         0.03624         0.02987         0.31097\n",
      "  1400       0.17         0.03555         0.02937         0.31312\n",
      "  1600       0.23         0.03514         0.02904         0.31473\n",
      "  1800       0.17         0.03484         0.02885         0.31484\n",
      "  2000       0.17         0.03460         0.02872         0.31459\n",
      "  2159       0.18         0.03449         0.02869         0.31468\n",
      "\n",
      "Terminated training for early stopping at Epoch 2159\n",
      "\n",
      "Total training time: 409.96 sec\n",
      "Loss:\n",
      "       Train   Val  Test\n",
      "$R^2$   0.91  0.91  0.58\n",
      "----------------------------------------------------------------\n",
      "MLP-Re\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       0.21     17091.92775     17170.82075       259.08397\n",
      "   200       0.18         0.80341         0.70258         2.74136\n",
      "   400       0.21         0.21521         0.18236         0.71764\n",
      "   600       0.19         0.10167         0.09127         0.44761\n",
      "   800       0.21         0.06666         0.06099         0.29255\n",
      "  1000       0.19         0.04521         0.03942         0.23366\n",
      "  1200       0.19         0.03048         0.02512         0.20732\n",
      "  1400       0.19         0.02181         0.01690         0.19405\n",
      "  1600       0.18         0.01716         0.01428         0.18943\n",
      "  1800       0.20         0.01387         0.01042         0.18709\n",
      "  2000       0.18         0.01227         0.00904         0.18555\n",
      "  2200       0.18         0.01060         0.00825         0.18443\n",
      "  2400       0.18         0.00965         0.00734         0.18257\n",
      "  2600       0.18         0.00913         0.00722         0.18230\n",
      "  2800       0.18         0.00857         0.00607         0.18221\n",
      "  3000       0.18         0.00775         0.00596         0.18213\n",
      "  3200       0.18         0.00724         0.00542         0.18207\n",
      "  3400       0.19         0.00683         0.00470         0.18386\n",
      "  3600       0.18         0.00637         0.00436         0.18480\n",
      "  3800       0.37         0.00598         0.00403         0.18587\n",
      "  4000       0.19         0.00567         0.00384         0.18654\n",
      "  4200       0.19         0.00555         0.00358         0.18845\n",
      "  4400       0.19         0.00531         0.00341         0.18882\n",
      "  4600       0.20         0.00518         0.00324         0.19121\n",
      "  4643       0.19         0.00516         0.00361         0.19084\n",
      "\n",
      "Terminated training for early stopping at Epoch 4643\n",
      "\n",
      "Total training time: 876.85 sec\n",
      "Loss:\n",
      "       Train   Val  Test\n",
      "$R^2$   0.99  0.99  0.75\n",
      "----------------------------------------------------------------\n",
      "MLP-BC-Re\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       0.26      9132.75683      9270.76467       184.47081\n",
      "   200       0.20         0.28016         0.24570         0.71471\n",
      "   400       0.21         0.10532         0.10129         0.30100\n",
      "   600       0.21         0.04755         0.04927         0.16828\n",
      "   800       0.20         0.01608         0.01622         0.11266\n",
      "  1000       0.20         0.00915         0.00888         0.09833\n",
      "  1200       0.21         0.00735         0.00645         0.09250\n",
      "  1400       0.20         0.00598         0.00539         0.08803\n",
      "  1600       0.21         0.00524         0.00522         0.08459\n",
      "  1800       0.20         0.00471         0.00421         0.08048\n",
      "  2000       0.19         0.00416         0.00382         0.07653\n",
      "  2200       0.19         0.00379         0.00369         0.07593\n",
      "  2400       0.19         0.00353         0.00310         0.07294\n",
      "  2600       0.19         0.00331         0.00294         0.07216\n",
      "  2800       0.23         0.00334         0.00279         0.06977\n",
      "  3000       0.19         0.00298         0.00265         0.06851\n",
      "  3200       0.19         0.00283         0.00244         0.06814\n",
      "  3400       0.19         0.00271         0.00244         0.06770\n",
      "  3544       0.19         0.00264         0.00241         0.06713\n",
      "\n",
      "Terminated training for early stopping at Epoch 3544\n",
      "\n",
      "Total training time: 713.53 sec\n",
      "Loss:\n",
      "       Train   Val  Test\n",
      "$R^2$   0.99  0.99  0.91\n",
      "----------------------------------------------------------------\n",
      "Bootstrap MLP-BC-Re Prior=1\n",
      "ensemble 1\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       0.40      1189.03656      1178.36897         9.09979\n",
      "   200       0.33         0.16127         0.14313         0.56018\n",
      "   400       0.33         0.05320         0.05564         0.17357\n",
      "   600       0.37         0.02437         0.02312         0.11221\n",
      "   800       0.34         0.01172         0.01033         0.09338\n",
      "  1000       0.34         0.00566         0.00483         0.06995\n",
      "\n",
      "Total training time: 351.40 sec\n",
      "Loss:\n",
      "       Train   Val  Test\n",
      "$R^2$   0.99  0.99  0.91\n",
      "----------------------------------------------------------------\n",
      "ensemble 2\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       0.57      6507.73795      6594.04406        76.55438\n",
      "   200       0.34         0.26455         0.22550         0.92222\n",
      "   400       0.35         0.05361         0.05394         0.17882\n",
      "   600       0.34         0.02137         0.01937         0.11122\n",
      "   800       0.44         0.01189         0.00973         0.10110\n",
      "  1000       0.42         0.00891         0.00729         0.09242\n",
      "\n",
      "Total training time: 398.63 sec\n",
      "Loss:\n",
      "       Train   Val  Test\n",
      "$R^2$   0.98  0.98  0.88\n",
      "----------------------------------------------------------------\n",
      "ensemble 3\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       0.48      9015.48104      9164.23812       114.89607\n",
      "   200       0.34         0.19947         0.17282         0.69432\n",
      "   400       0.87         0.05240         0.05892         0.16722\n",
      "   600       0.79         0.02455         0.02254         0.11264\n",
      "   800       0.44         0.01265         0.01120         0.09395\n",
      "  1000       0.49         0.00834         0.00734         0.08204\n",
      "\n",
      "Total training time: 455.83 sec\n",
      "Loss:\n",
      "       Train   Val  Test\n",
      "$R^2$   0.98  0.98  0.89\n",
      "----------------------------------------------------------------\n",
      "ensemble 4\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       0.61       283.69756       270.18495         0.96756\n",
      "   200       0.43         0.10664         0.10611         0.31333\n",
      "   400       0.44         0.04926         0.04951         0.15102\n",
      "   600       0.63         0.02227         0.02160         0.10634\n",
      "   800       0.46         0.01186         0.01055         0.09516\n",
      "  1000       0.42         0.00692         0.00654         0.07515\n",
      "\n",
      "Total training time: 515.60 sec\n",
      "Loss:\n",
      "       Train   Val  Test\n",
      "$R^2$   0.98  0.98   0.9\n",
      "----------------------------------------------------------------\n",
      "ensemble 5\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       0.44        15.78115         9.97267         1.42501\n",
      "   200       0.47         0.07867         0.08284         0.26159\n",
      "   400       0.79         0.02948         0.02804         0.13603\n",
      "   600       0.73         0.01264         0.01141         0.12813\n",
      "   800       0.69         0.00962         0.00840         0.11459\n",
      "  1000       0.43         0.00761         0.00645         0.09570\n",
      "\n",
      "Total training time: 574.03 sec\n",
      "Loss:\n",
      "       Train   Val  Test\n",
      "$R^2$   0.98  0.98  0.87\n",
      "----------------------------------------------------------------\n",
      "Bootstrap MLP-BC-Re Prior=2\n",
      "ensemble 1\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       0.45      1408.41235      1402.89380        43.41044\n",
      "   200       0.51         0.09570         0.09273         0.32867\n",
      "   400       0.72         0.04517         0.04416         0.17861\n",
      "   600       0.50         0.02470         0.02251         0.14865\n",
      "   800       0.56         0.01584         0.01384         0.13992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000       0.48         0.01316         0.01153         0.13241\n",
      "\n",
      "Total training time: 614.19 sec\n",
      "Loss:\n",
      "       Train   Val  Test\n",
      "$R^2$   0.97  0.97  0.83\n",
      "----------------------------------------------------------------\n",
      "ensemble 2\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       0.49       145.56868       123.43536        11.38736\n",
      "   200       0.50         0.07583         0.07674         0.24801\n",
      "   400       0.81         0.03905         0.03730         0.15882\n",
      "   600       0.48         0.02035         0.01795         0.13378\n",
      "   800       0.55         0.01264         0.01144         0.12438\n",
      "  1000       0.49         0.00979         0.00847         0.11575\n",
      "\n",
      "Total training time: 583.21 sec\n",
      "Loss:\n",
      "       Train   Val  Test\n",
      "$R^2$   0.97  0.97  0.85\n",
      "----------------------------------------------------------------\n",
      "ensemble 3\n",
      "\n",
      " epoch    time(s)      train_loss        val_loss       test_loss\n",
      "     0       0.58     13703.30571     14020.67814       184.78816\n",
      "   200       0.45         0.33190         0.28403         1.29743\n",
      "   400       0.46         0.05391         0.05436         0.18905\n"
     ]
    }
   ],
   "source": [
    "####################### MLP #########################\n",
    "print('MLP')\n",
    "MLP_loss=[]\n",
    "MLP_preds=[]\n",
    "\n",
    "MLP = ModelTrainer()\n",
    "MLP.set_target_variable(target_label)\n",
    "MLP.set_model(model_MLP, **model_MLP_kwargs)\n",
    "MLP.set_criterion(nn.MSELoss())\n",
    "MLP.set_optimizer('Adam', lr=1e-6)\n",
    "MLP.set_config(batch_size=10, print_freq=200, max_epochs=10000, earlystopping=True, patience=30)\n",
    "   \n",
    "MLP_loss.append(MLP.fit(df_train, df_val, df_test))\n",
    "    \n",
    "preds, targets = MLP.predict(df)\n",
    "MLP_preds.append(preds)\n",
    "    \n",
    "scores = {'Train': evaluate(MLP, df_train),\n",
    "          'Val': evaluate(MLP, df_val),\n",
    "          'Test': evaluate(MLP, df_test)}\n",
    "print('Loss:')\n",
    "print(pd.DataFrame(scores).round(2))\n",
    "print('----------------------------------------------------------------')\n",
    "\n",
    "MLP_preds = np.array(MLP_preds)\n",
    "MLP_mean = MLP_preds.mean(axis=0)\n",
    "MLP_std = MLP_preds.std(axis=0)\n",
    "MLP_mean = MLP_mean.reshape(len(MLP_mean),)\n",
    "MLP_std = MLP_std.reshape(len(MLP_std),)\n",
    "\n",
    "####################### MLP-Re #########################\n",
    "print('MLP-Re')\n",
    "MLP_Re_loss=[]\n",
    "MLP_Re_preds=[]\n",
    "\n",
    "MLP_Re = ModelTrainer()\n",
    "MLP_Re.set_target_variable(target_label)\n",
    "MLP_Re.set_model(model_MLP_Re, **model_MLP_Re_kwargs)\n",
    "MLP_Re.set_criterion(nn.MSELoss())\n",
    "MLP_Re.set_optimizer('Adam', lr=1e-6)\n",
    "MLP_Re.set_config(batch_size=10, print_freq=200, max_epochs=10000, earlystopping=True, patience=30)\n",
    "   \n",
    "MLP_Re_loss.append(MLP_Re.fit(df_train, df_val, df_test))\n",
    "    \n",
    "preds, targets = MLP_Re.predict(df)\n",
    "MLP_Re_preds.append(preds)\n",
    "    \n",
    "scores = {'Train': evaluate(MLP_Re, df_train),\n",
    "          'Val': evaluate(MLP_Re, df_val),\n",
    "          'Test': evaluate(MLP_Re, df_test)}\n",
    "print('Loss:')\n",
    "print(pd.DataFrame(scores).round(2))\n",
    "print('----------------------------------------------------------------')\n",
    "\n",
    "MLP_Re_preds = np.array(MLP_Re_preds)\n",
    "MLP_Re_mean = MLP_Re_preds.mean(axis=0)\n",
    "MLP_Re_std = MLP_Re_preds.std(axis=0)\n",
    "MLP_Re_mean = MLP_Re_mean.reshape(len(MLP_Re_mean),)\n",
    "MLP_Re_std = MLP_Re_std.reshape(len(MLP_Re_std),)\n",
    "\n",
    "####################### MLP-BC-Re #########################\n",
    "print('MLP-BC-Re')\n",
    "MLP_BC_Re_loss=[]\n",
    "MLP_BC_Re_preds=[]\n",
    "\n",
    "MLP_BC_Re = ModelTrainer()\n",
    "MLP_BC_Re.set_target_variable(target_label)\n",
    "MLP_BC_Re.set_model(model_MLP_BC_Re, **model_MLP_BC_Re_kwargs)\n",
    "MLP_BC_Re.set_criterion(nn.MSELoss())\n",
    "MLP_BC_Re.set_optimizer('Adam', lr=1e-6)\n",
    "MLP_BC_Re.set_config(batch_size=10, print_freq=200, max_epochs=10000, earlystopping=True, patience=30)\n",
    "   \n",
    "MLP_BC_Re_loss.append(MLP_BC_Re.fit(df_train, df_val, df_test))\n",
    "    \n",
    "preds, targets = MLP_BC_Re.predict(df)\n",
    "MLP_BC_Re_preds.append(preds)\n",
    "    \n",
    "scores = {'Train': evaluate(MLP_BC_Re, df_train),\n",
    "          'Val': evaluate(MLP_BC_Re, df_val),\n",
    "          'Test': evaluate(MLP_BC_Re, df_test)}\n",
    "print('Loss:')\n",
    "print(pd.DataFrame(scores).round(2))\n",
    "print('----------------------------------------------------------------')\n",
    "\n",
    "MLP_BC_Re_preds = np.array(MLP_BC_Re_preds)\n",
    "MLP_BC_Re_mean = MLP_BC_Re_preds.mean(axis=0)\n",
    "MLP_BC_Re_std = MLP_BC_Re_preds.std(axis=0)\n",
    "MLP_BC_Re_mean = MLP_BC_Re_mean.reshape(len(MLP_BC_Re_mean),)\n",
    "MLP_BC_Re_std = MLP_BC_Re_std.reshape(len(MLP_BC_Re_std),)\n",
    "\n",
    "####################### MLP-BC-Re-NL #########################\n",
    "#print('MLP-BC-Re-NL')\n",
    "#MLP_BC_Re_NL_loss=[]\n",
    "#MLP_BC_Re_NL_preds=[]\n",
    "\n",
    "#MLP_BC_Re_NL = ModelTrainer()\n",
    "#MLP_BC_Re_NL.set_target_variable(target_label)\n",
    "#MLP_BC_Re_NL.set_model(model_MLP_BC_Re_NL, **model_MLP_BC_Re_NL_kwargs)\n",
    "#MLP_BC_Re_NL.set_criterion(nn.MSELoss())\n",
    "#MLP_BC_Re_NL.set_optimizer('Adam', lr=1e-6)\n",
    "#MLP_BC_Re_NL.set_config(batch_size=10, print_freq=200, max_epochs=1000, earlystopping=True, patience=30)\n",
    "   \n",
    "#MLP_BC_Re_NL_loss.append(MLP_BC_Re_NL.fit(df_train, df_val, df_test))\n",
    "    \n",
    "#preds, targets = MLP_BC_Re_NL.predict(df)\n",
    "#MLP_BC_Re_NL_preds.append(preds)\n",
    "    \n",
    "#scores = {'Train': evaluate(MLP_BC_Re_NL, df_train),\n",
    "#          'Val': evaluate(MLP_BC_Re_NL, df_val),\n",
    "#          'Test': evaluate(MLP_BC_Re_NL, df_test)}\n",
    "#print('Loss:')\n",
    "#print(pd.DataFrame(scores).round(2))\n",
    "#print('----------------------------------------------------------------')\n",
    "\n",
    "#MLP_BC_Re_NL_preds = np.array(MLP_BC_Re_NL_preds)\n",
    "#MLP_BC_Re_NL_mean = MLP_BC_Re_NL_preds.mean(axis=0)\n",
    "#MLP_BC_Re_NL_std = MLP_BC_Re_NL_preds.std(axis=0)\n",
    "#MLP_BC_Re_NL_mean = MLP_BC_Re_NL_mean.reshape(len(MLP_BC_Re_NL_mean),)\n",
    "#MLP_BC_Re_NL_std = MLP_BC_Re_NL_std.reshape(len(MLP_BC_Re_NL_std),)\n",
    "\n",
    "####################### Bootstrap MLP-BC-Re-Prior=1 #########################\n",
    "print('Bootstrap MLP-BC-Re Prior=1')\n",
    "BootMLP_Prior1_loss=[]\n",
    "BootMLP_Prior1_preds=[]\n",
    "BootMLP_Prior1_targets=[]\n",
    "\n",
    "ensemble = 5\n",
    "for i in range(ensemble):\n",
    "    print('ensemble {}'.format(i+1))\n",
    "    BootMLP_Prior1 = ModelTrainer()\n",
    "    BootMLP_Prior1.set_target_variable(target_label)\n",
    "    BootMLP_Prior1.set_model(model1, **model1_kwargs)\n",
    "    BootMLP_Prior1.set_criterion(nn.MSELoss())\n",
    "    BootMLP_Prior1.set_optimizer('Adam', lr=1e-6)\n",
    "    BootMLP_Prior1.set_config(batch_size=10, print_freq=200, max_epochs=1000, earlystopping=True, patience=30)\n",
    "    \n",
    "    BootMLP_Prior1_loss.append(BootMLP_Prior1.fit(df_train, df_val, df_test))\n",
    "    \n",
    "    preds, targets = BootMLP_Prior1.predict(df)\n",
    "    BootMLP_Prior1_preds.append(preds)\n",
    "    BootMLP_Prior1_targets.append(targets)\n",
    "    \n",
    "    scores = {'Train': evaluate(BootMLP_Prior1, df_train),\n",
    "              'Val': evaluate(BootMLP_Prior1, df_val),\n",
    "              'Test': evaluate(BootMLP_Prior1, df_test)}\n",
    "    print('Loss:')\n",
    "    print(pd.DataFrame(scores).round(2))\n",
    "    print('----------------------------------------------------------------')\n",
    "\n",
    "BootMLP_Prior1_preds = np.array(BootMLP_Prior1_preds)\n",
    "BootMLP_Prior1_mean = BootMLP_Prior1_preds.mean(axis=0)\n",
    "BootMLP_Prior1_std = BootMLP_Prior1_preds.std(axis=0)\n",
    "BootMLP_Prior1_mean = BootMLP_Prior1_mean.reshape(len(BootMLP_Prior1_mean),)\n",
    "BootMLP_Prior1_std = BootMLP_Prior1_std.reshape(len(BootMLP_Prior1_std),)\n",
    "\n",
    "####################### Bootstrap MLP-BC-Re-Prior=2 #########################\n",
    "print('Bootstrap MLP-BC-Re Prior=2')\n",
    "BootMLP_Prior2_loss=[]\n",
    "BootMLP_Prior2_preds=[]\n",
    "BootMLP_Prior2_targets=[]\n",
    "\n",
    "ensemble = 5\n",
    "for i in range(ensemble):\n",
    "    print('ensemble {}'.format(i+1))\n",
    "    BootMLP_Prior2 = ModelTrainer()\n",
    "    BootMLP_Prior2.set_target_variable(target_label)\n",
    "    BootMLP_Prior2.set_model(model2, **model2_kwargs)\n",
    "    BootMLP_Prior2.set_criterion(nn.MSELoss())\n",
    "    BootMLP_Prior2.set_optimizer('Adam', lr=1e-6)\n",
    "    BootMLP_Prior2.set_config(batch_size=10, print_freq=200, max_epochs=1000, earlystopping=True, patience=30)\n",
    "    \n",
    "    BootMLP_Prior2_loss.append(BootMLP_Prior2.fit(df_train, df_val, df_test))\n",
    "    \n",
    "    preds, targets = BootMLP_Prior2.predict(df)\n",
    "    BootMLP_Prior2_preds.append(preds)\n",
    "    BootMLP_Prior2_targets.append(targets)\n",
    "    \n",
    "    scores = {'Train': evaluate(BootMLP_Prior2, df_train),\n",
    "              'Val': evaluate(BootMLP_Prior2, df_val),\n",
    "              'Test': evaluate(BootMLP_Prior2, df_test)}\n",
    "    print('Loss:')\n",
    "    print(pd.DataFrame(scores).round(2))\n",
    "    print('----------------------------------------------------------------')\n",
    "\n",
    "BootMLP_Prior2_preds = np.array(BootMLP_Prior2_preds)\n",
    "BootMLP_Prior2_mean = BootMLP_Prior2_preds.mean(axis=0)\n",
    "BootMLP_Prior2_std = BootMLP_Prior2_preds.std(axis=0)\n",
    "BootMLP_Prior2_mean = BootMLP_Prior2_mean.reshape(len(BootMLP_Prior2_mean),)\n",
    "BootMLP_Prior2_std = BootMLP_Prior2_std.reshape(len(BootMLP_Prior2_std),)\n",
    "\n",
    "####################### Bootstrap MLP-BC-Re-Prior=3 #########################\n",
    "print('Bootstrap MLP-BC-Re Prior=3')\n",
    "BootMLP_Prior3_loss=[]\n",
    "BootMLP_Prior3_preds=[]\n",
    "BootMLP_Prior3_targets=[]\n",
    "\n",
    "ensemble = 5\n",
    "for i in range(ensemble):\n",
    "    print('ensemble {}'.format(i+1))\n",
    "    BootMLP_Prior3 = ModelTrainer()\n",
    "    BootMLP_Prior3.set_target_variable(target_label)\n",
    "    BootMLP_Prior3.set_model(model3, **model3_kwargs)\n",
    "    BootMLP_Prior3.set_criterion(nn.MSELoss())\n",
    "    BootMLP_Prior3.set_optimizer('Adam', lr=1e-6)\n",
    "    BootMLP_Prior3.set_config(batch_size=10, print_freq=200, max_epochs=10000, earlystopping=True, patience=30)\n",
    "    \n",
    "    BootMLP_Prior3_loss.append(BootMLP_Prior3.fit(df_train, df_val, df_test))\n",
    "    \n",
    "    preds, targets = BootMLP_Prior3.predict(df)\n",
    "    BootMLP_Prior3_preds.append(preds)\n",
    "    BootMLP_Prior3_targets.append(targets)\n",
    "    \n",
    "    scores = {'Train': evaluate(BootMLP_Prior3, df_train),\n",
    "              'Val': evaluate(BootMLP_Prior3, df_val),\n",
    "              'Test': evaluate(BootMLP_Prior3, df_test)}\n",
    "    print('Loss:')\n",
    "    print(pd.DataFrame(scores).round(2))\n",
    "    print('----------------------------------------------------------------')\n",
    "\n",
    "BootMLP_Prior3_preds = np.array(BootMLP_Prior3_preds)\n",
    "BootMLP_Prior3_mean = BootMLP_Prior3_preds.mean(axis=0)\n",
    "BootMLP_Prior3_std = BootMLP_Prior3_preds.std(axis=0)\n",
    "BootMLP_Prior3_mean = BootMLP_Prior3_mean.reshape(len(BootMLP_Prior3_mean),)\n",
    "BootMLP_Prior3_std = BootMLP_Prior3_std.reshape(len(BootMLP_Prior3_std),)\n",
    "\n",
    "####################### Bootstrap MLP-BC-Re-Prior=5 #########################\n",
    "print('Bootstrap MLP-BC-Re Prior=5')\n",
    "BootMLP_Prior5_loss=[]\n",
    "BootMLP_Prior5_preds=[]\n",
    "BootMLP_Prior5_targets=[]\n",
    "\n",
    "ensemble = 5\n",
    "for i in range(ensemble):\n",
    "    print('ensemble {}'.format(i+1))\n",
    "    BootMLP_Prior5 = ModelTrainer()\n",
    "    BootMLP_Prior5.set_target_variable(target_label)\n",
    "    BootMLP_Prior5.set_model(model5, **model5_kwargs)\n",
    "    BootMLP_Prior5.set_criterion(nn.MSELoss())\n",
    "    BootMLP_Prior5.set_optimizer('Adam', lr=1e-6)\n",
    "    BootMLP_Prior5.set_config(batch_size=10, print_freq=200, max_epochs=1000, earlystopping=True, patience=30)\n",
    "    \n",
    "    BootMLP_Prior5_loss.append(BootMLP_Prior5.fit(df_train, df_val, df_test))\n",
    "    \n",
    "    preds, targets = BootMLP_Prior5.predict(df)\n",
    "    BootMLP_Prior5_preds.append(preds)\n",
    "    BootMLP_Prior5_targets.append(targets)\n",
    "    \n",
    "    scores = {'Train': evaluate(BootMLP_Prior5, df_train),\n",
    "              'Val': evaluate(BootMLP_Prior5, df_val),\n",
    "              'Test': evaluate(BootMLP_Prior5, df_test)}\n",
    "    print('Loss:')\n",
    "    print(pd.DataFrame(scores).round(2))\n",
    "    print('----------------------------------------------------------------')\n",
    "\n",
    "BootMLP_Prior5_preds = np.array(BootMLP_Prior5_preds)\n",
    "BootMLP_Prior5_mean = BootMLP_Prior5_preds.mean(axis=0)\n",
    "BootMLP_Prior5_std = BootMLP_Prior5_preds.std(axis=0)\n",
    "BootMLP_Prior5_mean = BootMLP_Prior5_mean.reshape(len(BootMLP_Prior5_mean),)\n",
    "BootMLP_Prior5_std = BootMLP_Prior5_std.reshape(len(BootMLP_Prior5_std),)\n",
    "\n",
    "####################### Bootstrap MLP-BC-Re-Prior=7 #########################\n",
    "print('Bootstrap MLP-BC-Re Prior=7')\n",
    "BootMLP_Prior7_loss=[]\n",
    "BootMLP_Prior7_preds=[]\n",
    "BootMLP_Prior7_targets=[]\n",
    "\n",
    "ensemble = 5\n",
    "for i in range(ensemble):\n",
    "    print('ensemble {}'.format(i+1))\n",
    "    BootMLP_Prior7 = ModelTrainer()\n",
    "    BootMLP_Prior7.set_target_variable(target_label)\n",
    "    BootMLP_Prior7.set_model(model7, **model7_kwargs)\n",
    "    BootMLP_Prior7.set_criterion(nn.MSELoss())\n",
    "    BootMLP_Prior7.set_optimizer('Adam', lr=1e-6)\n",
    "    BootMLP_Prior7.set_config(batch_size=10, print_freq=200, max_epochs=1000, earlystopping=True, patience=30)\n",
    "    \n",
    "    BootMLP_Prior7_loss.append(BootMLP_Prior7.fit(df_train, df_val, df_test))\n",
    "    \n",
    "    preds, targets = BootMLP_Prior7.predict(df)\n",
    "    BootMLP_Prior7_preds.append(preds)\n",
    "    BootMLP_Prior7_targets.append(targets)\n",
    "    \n",
    "    scores = {'Train': evaluate(BootMLP_Prior7, df_train),\n",
    "              'Val': evaluate(BootMLP_Prior7, df_val),\n",
    "              'Test': evaluate(BootMLP_Prior7, df_test)}\n",
    "    print('Loss:')\n",
    "    print(pd.DataFrame(scores).round(2))\n",
    "    print('----------------------------------------------------------------')\n",
    "\n",
    "BootMLP_Prior7_preds = np.array(BootMLP_Prior7_preds)\n",
    "BootMLP_Prior7_mean = BootMLP_Prior7_preds.mean(axis=0)\n",
    "BootMLP_Prior7_std = BootMLP_Prior7_preds.std(axis=0)\n",
    "BootMLP_Prior7_mean = BootMLP_Prior7_mean.reshape(len(BootMLP_Prior7_mean),)\n",
    "BootMLP_Prior7_std = BootMLP_Prior7_std.reshape(len(BootMLP_Prior7_std),)\n",
    "\n",
    "####################### Bootstrap MLP-BC-Re-Prior=10 #########################\n",
    "print('Bootstrap MLP-BC-Re Prior=10')\n",
    "BootMLP_Prior10_loss=[]\n",
    "BootMLP_Prior10_preds=[]\n",
    "BootMLP_Prior10_targets=[]\n",
    "\n",
    "ensemble = 5\n",
    "for i in range(ensemble):\n",
    "    print('ensemble {}'.format(i+1))\n",
    "    BootMLP_Prior10 = ModelTrainer()\n",
    "    BootMLP_Prior10.set_target_variable(target_label)\n",
    "    BootMLP_Prior10.set_model(model10, **model10_kwargs)\n",
    "    BootMLP_Prior10.set_criterion(nn.MSELoss())\n",
    "    BootMLP_Prior10.set_optimizer('Adam', lr=1e-6)\n",
    "    BootMLP_Prior10.set_config(batch_size=10, print_freq=200, max_epochs=1000, earlystopping=True, patience=30)\n",
    "    \n",
    "    BootMLP_Prior10_loss.append(BootMLP_Prior10.fit(df_train, df_val, df_test))\n",
    "    \n",
    "    preds, targets = BootMLP_Prior10.predict(df)\n",
    "    BootMLP_Prior10_preds.append(preds)\n",
    "    BootMLP_Prior10_targets.append(targets)\n",
    "    \n",
    "    scores = {'Train': evaluate(BootMLP_Prior10, df_train),\n",
    "              'Val': evaluate(BootMLP_Prior10, df_val),\n",
    "              'Test': evaluate(BootMLP_Prior10, df_test)}\n",
    "    print('Loss:')\n",
    "    print(pd.DataFrame(scores).round(2))\n",
    "    print('----------------------------------------------------------------')\n",
    "\n",
    "BootMLP_Prior10_preds = np.array(BootMLP_Prior10_preds)\n",
    "BootMLP_Prior10_mean = BootMLP_Prior10_preds.mean(axis=0)\n",
    "BootMLP_Prior10_std = BootMLP_Prior10_preds.std(axis=0)\n",
    "BootMLP_Prior10_mean = BootMLP_Prior10_mean.reshape(len(BootMLP_Prior10_mean),)\n",
    "BootMLP_Prior10_std = BootMLP_Prior10_std.reshape(len(BootMLP_Prior10_std),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,10], dpi=300)\n",
    "fig.suptitle('Uncertainty Estimates by Bootstrapped Ensemble', verticalalignment='center')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior1_mean, 'b--', linewidth=1.5, label='MLP-BC-Re-Prior1 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior1_mean - BootMLP_Prior1_std, BootMLP_Prior1_mean + BootMLP_Prior1_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior1_mean + 1.64*BootMLP_Prior1_std, BootMLP_Prior1_mean - 1.64*BootMLP_Prior1_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior1_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(df['y+'], targets, 'k.',  label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior1_mean, 'b--', linewidth=1.5, label='MLP-BC-Re-Prior1 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior1_mean - BootMLP_Prior1_std, BootMLP_Prior1_mean + BootMLP_Prior1_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior1_mean + 1.64*BootMLP_Prior1_std, BootMLP_Prior1_mean - 1.64*BootMLP_Prior1_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 stdinty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel('y+'); plt.ylabel('log($a_{uv}$)')\n",
    "plt.xlim(-100,6000)\n",
    "plt.ylim(-4, 0.5)\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$log(a_{uv})$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior1_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Loss for MLP')\n",
    "fig = plt.figure(dpi=300)\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_loss_history(MLP_loss[0], loglog=True)\n",
    "plt.xlim(-20, 1000)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_loss_history(MLP_loss[0], loglog=True)\n",
    "plt.xlim(-20, 300)\n",
    "\n",
    "\n",
    "print('Loss for MLP-Re')\n",
    "fig = plt.figure(dpi=300)\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_loss_history(MLP_Re_loss[0], loglog=True)\n",
    "plt.xlim(-20, 1000)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_loss_history(MLP_Re_loss[0], loglog=True)\n",
    "plt.xlim(-20, 100)\n",
    "\n",
    "\n",
    "print('Loss for MLP-BC-Re')\n",
    "fig = plt.figure(dpi=300)\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_loss_history(MLP_BC_Re_loss[0], loglog=True)\n",
    "plt.xlim(-20, 1000)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_loss_history(MLP_BC_Re_loss[0], loglog=True)\n",
    "plt.xlim(-20, 100)\n",
    "\n",
    "\n",
    "#print('Loss for MLP-BC-Re-NL')\n",
    "#fig = plt.figure(dpi=300)\n",
    "#plt.subplot(2, 1, 1)\n",
    "#for i in range(ensemble):\n",
    "#    plot_loss_history(MLP_BC_Re_NL_loss[i], loglog=False)\n",
    "#plt.xlim(-20, 1000)\n",
    "\n",
    "#plt.subplot(2, 1, 2)\n",
    "#for i in range(ensemble):\n",
    "#    plot_loss_history(MLP_BC_Re_NL_loss[i], loglog=False)\n",
    "#plt.xlim(-20, 100)\n",
    "\n",
    "\n",
    "print('Loss for Bootstrap for MLP-BC-Re-Prior1')\n",
    "fig = plt.figure(dpi=300)\n",
    "plt.subplot(2, 1, 1)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior1_loss[i], loglog=True)\n",
    "plt.xlim(-20, 1000)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior1_loss[i], loglog=True)\n",
    "plt.xlim(-20, 100)\n",
    "\n",
    "print('Loss for Bootstrap for MLP-BC-Re-Prior2')\n",
    "fig = plt.figure(dpi=300)\n",
    "plt.subplot(2, 1, 1)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior2_loss[i], loglog=True)\n",
    "plt.xlim(-20, 1000)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior2_loss[i], loglog=True)\n",
    "plt.xlim(-20, 100)\n",
    "\n",
    "print('Loss for Bootstrap for MLP-BC-Re-Prior3')\n",
    "fig = plt.figure(dpi=300)\n",
    "plt.subplot(2, 1, 1)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior3_loss[i], loglog=True)\n",
    "plt.xlim(-20, 1000)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior3_loss[i], loglog=True)\n",
    "plt.xlim(-20, 100)\n",
    "\n",
    "print('Loss for Bootstrap for MLP-BC-Re-Prior5')\n",
    "fig = plt.figure(dpi=300)\n",
    "plt.subplot(2, 1, 1)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior5_loss[i], loglog=True)\n",
    "plt.xlim(-20, 1000)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior5_loss[i], loglog=True)\n",
    "plt.xlim(-20, 100)\n",
    "\n",
    "print('Loss for Bootstrap for MLP-BC-Re-Prior7')\n",
    "fig = plt.figure(dpi=300)\n",
    "plt.subplot(2, 1, 1)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior7_loss[i], loglog=True)\n",
    "plt.xlim(-20, 1000)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior7_loss[i], loglog=True)\n",
    "plt.xlim(-20, 100)\n",
    "\n",
    "print('Loss for Bootstrap for MLP-BC-Re-Prior10')\n",
    "fig = plt.figure(dpi=300)\n",
    "plt.subplot(2, 1, 1)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior10_loss[i], loglog=True)\n",
    "plt.xlim(-20, 1000)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range(ensemble):\n",
    "    plot_loss_history(BootMLP_Prior10_loss[i], loglog=True)\n",
    "plt.xlim(-20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "MLP.save_checkpoint('MLPcheckpoint.pt')\n",
    "MLP_Re.save_checkpoint('MLP_Re_checkpoint.pt')\n",
    "MLP_BC_Re.save_checkpoint('MLP_BC_Re_checkpoint.pt')\n",
    "BootMLP_Prior1.save_checkpoint('MLP_BC_Re_Prior1_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prior=2')\n",
    "fig = plt.figure(figsize=[12,10], dpi=300)\n",
    "fig.suptitle('Uncertainty Estimates by Bootstrapped Ensemble', verticalalignment='center')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior2_mean, 'b--', linewidth=1.5, label='MLP-BC-Re Prior=2 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior2_mean - BootMLP_Prior2_std, BootMLP_Prior2_mean + BootMLP_Prior2_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior2_mean + 1.64*BootMLP_Prior2_std, BootMLP_Prior2_mean - 1.64*BootMLP_Prior2_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior2_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(df['y+'], targets, 'k.',  label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior2_mean, 'b--', linewidth=1.5, label='MLP-BC-Re Prior=2 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior2_mean - BootMLP_Prior2_std, BootMLP_Prior2_mean + BootMLP_Prior2_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior2_mean + 1.64*BootMLP_Prior2_std, BootMLP_Prior2_mean - 1.64*BootMLP_Prior2_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 stdinty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel('y+'); plt.ylabel('log($a_{uv}$)')\n",
    "plt.xlim(-100,6000)\n",
    "plt.ylim(-4, 0.5)\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$log(a_{uv})$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior2_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Prior=3')\n",
    "fig = plt.figure(figsize=[12,10], dpi=300)\n",
    "fig.suptitle('Uncertainty Estimates by Bootstrapped Ensemble', verticalalignment='center')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior3_mean, 'b--', linewidth=1.5, label='MLP-BC-Re Prior=3 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior3_mean - BootMLP_Prior3_std, BootMLP_Prior3_mean + BootMLP_Prior3_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior3_mean + 1.64*BootMLP_Prior3_std, BootMLP_Prior3_mean - 1.64*BootMLP_Prior3_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior3_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(df['y+'], targets, 'k.',  label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior3_mean, 'b--', linewidth=1.5, label='MLP-BC-Re Prior=3 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior3_mean - BootMLP_Prior3_std, BootMLP_Prior3_mean + BootMLP_Prior3_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior3_mean + 1.64*BootMLP_Prior3_std, BootMLP_Prior3_mean - 1.64*BootMLP_Prior3_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 stdinty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel('y+'); plt.ylabel('log($a_{uv}$)')\n",
    "plt.xlim(-100,6000)\n",
    "plt.ylim(-4, 0.5)\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$log(a_{uv})$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior3_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prior=5')\n",
    "fig = plt.figure(figsize=[12,10], dpi=300)\n",
    "fig.suptitle('Uncertainty Estimates by Bootstrapped Ensemble', verticalalignment='center')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior2_mean, 'b--', linewidth=1.5, label='MLP-BC-Re Prior=5 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior5_mean - BootMLP_Prior5_std, BootMLP_Prior5_mean + BootMLP_Prior5_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior5_mean + 1.64*BootMLP_Prior5_std, BootMLP_Prior5_mean - 1.64*BootMLP_Prior5_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior5_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(df['y+'], targets, 'k.',  label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior5_mean, 'b--', linewidth=1.5, label='MLP-BC-Re Prior=5 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior5_mean - BootMLP_Prior5_std, BootMLP_Prior5_mean + BootMLP_Prior5_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior5_mean + 1.64*BootMLP_Prior5_std, BootMLP_Prior5_mean - 1.64*BootMLP_Prior5_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 stdinty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel('y+'); plt.ylabel('log($a_{uv}$)')\n",
    "plt.xlim(-100,6000)\n",
    "plt.ylim(-4, 0.5)\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$log(a_{uv})$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior5_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prior=7')\n",
    "fig = plt.figure(figsize=[12,10], dpi=300)\n",
    "fig.suptitle('Uncertainty Estimates by Bootstrapped Ensemble', verticalalignment='center')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior7_mean, 'b--', linewidth=1.5, label='MLP-BC-Re Prior=7 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior7_mean - BootMLP_Prior7_std, BootMLP_Prior7_mean + BootMLP_Prior7_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior7_mean + 1.64*BootMLP_Prior7_std, BootMLP_Prior7_mean - 1.64*BootMLP_Prior7_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior7_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(df['y+'], targets, 'k.',  label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior7_mean, 'b--', linewidth=1.5, label='MLP-BC-Re Prior=7 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior7_mean - BootMLP_Prior7_std, BootMLP_Prior7_mean + BootMLP_Prior7_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior7_mean + 1.64*BootMLP_Prior7_std, BootMLP_Prior7_mean - 1.64*BootMLP_Prior7_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 stdinty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel('y+'); plt.ylabel('log($a_{uv}$)')\n",
    "plt.xlim(-100,6000)\n",
    "plt.ylim(-4, 0.5)\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$log(a_{uv})$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior7_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prior=10')\n",
    "fig = plt.figure(figsize=[12,10], dpi=300)\n",
    "fig.suptitle('Uncertainty Estimates by Bootstrapped Ensemble', verticalalignment='center')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior10_mean, 'b--', linewidth=1.5, label='MLP-BC-Re Prior=10 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior10_mean - BootMLP_Prior10_std, BootMLP_Prior10_mean + BootMLP_Prior10_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior10_mean + 1.64*BootMLP_Prior10_std, BootMLP_Prior10_mean - 1.64*BootMLP_Prior10_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$a_{uv}$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior10_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5)\n",
    "\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(df['y+'], targets, 'k.',  label='DNS')\n",
    "plt.title('Predicted Mean with 90% Uncertainty Bound', fontsize=12)\n",
    "# predictive mean and deviation\n",
    "plt.plot(df['y+'], MLP_mean, 'k-', linewidth=1.5, label='MLP Prediction')\n",
    "plt.plot(df['y+'], MLP_Re_mean, 'g:', linewidth=1.5, label='MLP-Re Prediction')\n",
    "plt.plot(df['y+'], MLP_BC_Re_mean, 'c:', linewidth=1.5, label='MLP-BC-Re Prediction')\n",
    "#plt.plot(df['y+'], MLP_BC_Re_NL_mean, 'm:', linewidth=1.5, label='MLP-BC-Re-NL Prediction')\n",
    "plt.plot(df['y+'], BootMLP_Prior10_mean, 'b--', linewidth=1.5, label='MLP-BC-Re Prior=10 Prediction')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior10_mean - BootMLP_Prior10_std, BootMLP_Prior10_mean + BootMLP_Prior10_std, alpha=0.5, color='red')\n",
    "plt.fill_between(df['y+'], BootMLP_Prior10_mean + 1.64*BootMLP_Prior10_std, BootMLP_Prior10_mean - 1.64*BootMLP_Prior10_std, alpha=0.15, color='red') # 90% uncertainty: 1.64 stdinty: 1.64 std\n",
    "plt.legend(loc='lower left')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel('y+'); plt.ylabel('log($a_{uv}$)')\n",
    "plt.xlim(-100,6000)\n",
    "plt.ylim(-4, 0.5)\n",
    "###############################################################################\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(df['y+'], targets, 'k.', label='DNS')\n",
    "plt.title('Prediction of Each Ensemble', fontsize=12)\n",
    "plt.xlabel('y+'); plt.ylabel('$log(a_{uv})$')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# loop for predictions of each head \n",
    "for i in range(ensemble):\n",
    "    plt.plot(df['y+'], BootMLP_Prior10_preds[i,:], linestyle='--', linewidth=1.5)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim(-100, 800)\n",
    "plt.ylim(-4, 0.5);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
